{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA General Framework Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "from sklearn import ensemble, tree, linear_model\n",
    "#import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium general article\n",
    "https://towardsdatascience.com/exploratory-data-analysis-in-python-c9a77dfa39ce\n",
    "\n",
    "1. Import libraries\n",
    "2. Load df + head/tail\n",
    "3. Check datatypes\n",
    "4. Drop irrelevant cols\n",
    "5. Rename cols\n",
    "6. Drop duplicate rows\n",
    "7. Drop (or fill) missing/null values\n",
    "8. Detect outliers w/ visualization\n",
    "9. Plot features (hist, scatter, heatmap, ...)\n",
    "\n",
    "-> Full visualization tutorial: https://towardsdatascience.com/a-step-by-step-guide-for-creating-advanced-python-data-visualizations-with-seaborn-matplotlib-1579d6a1a7d0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "### Kaggle House Prices EDA\n",
    "https://www.kaggle.com/pavansanagapati/a-simple-tutorial-on-exploratory-data-analysis\n",
    "\n",
    "Nice conceptual intro + types and steps EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "Import data and extract basic stats, plus required preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT DATA\n",
    "\n",
    "path_train_data = ''\n",
    "path_test_data = ''\n",
    "\n",
    "df_train = pd.read_csv(path_train_data)\n",
    "df_test = pd.read_csv(path_test_data)\n",
    "\n",
    "# BASIC STATS \n",
    "\n",
    "df_train.shape\n",
    "df_test.shape\n",
    "\n",
    "df_train.head()\n",
    "df_train.tail()\n",
    "\n",
    "df_test.head()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target\n",
    "\n",
    "y_name = ''\n",
    "y = df_train[y_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK DATA TYPES\n",
    "\n",
    "df_train.dtypes\n",
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data type of some variables, especially dates, does not match with the proper one, it is better to change it right away.\n",
    "\n",
    "Error might occur if the columns contain NaNs depending on the method.\n",
    "\n",
    "For a more complete guide: https://stackoverflow.com/questions/15891038/change-data-type-of-columns-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date\n",
    "date_cols = []\n",
    "date_format = ''\n",
    "\n",
    "df_train[date_cols] = pd.to_datetime(df_train[date_cols], format=date_format)\n",
    "df_test[date_cols] = pd.to_datetime(df_test[date_cols], format=date_format)\n",
    "\n",
    "# Otherwise use .astype and the desired datatype \n",
    "# (from standard library, numpy, pandas, ...)\n",
    "int_cols = []\n",
    "\n",
    "df_train[int_cols] = df_train[int_cols].astype(int)\n",
    "df_test[int_cols] = df_test[int_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS DESCRIPTIVE STATS (Both train and test?)\n",
    "\n",
    "df_train.describe()\n",
    "df_test.describe()\n",
    "\n",
    "# What else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT NUMERIC, CATEGORICAL AND DATES (Only train because need label)\n",
    "# TODO: what about EDA within test data, ex. covariance matrix?\n",
    "\n",
    "numeric_features = df_train.select_dtypes(include=np.number)\n",
    "\n",
    "categorical_features = df_train.select_dtypes(include=np.object)\n",
    "\n",
    "datetime_features =  df_train.select_dtypes(include=['datetime64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features.head()\n",
    "categorical_features.head()\n",
    "datetime_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas EDA with dates?\n",
    "\n",
    "1. If there are more dates, it might more meaningful to use the difference (or other manipulations) between the dates to explain the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric data: discrete vs continuous\n",
    "\n",
    "In order to separate discrete from continuous variables, we can use an arbitrary treshold (ex. 25) for the unique values of the variable. This decision rule should always work and will not require manual separation based on the nature of the variables if the dataset is large enough.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold = 25\n",
    "\n",
    "discrete_features = [feature for feature in numeric_feature if len(df_train[feature].unique())<treshold]\n",
    "continuous_features = [feature for feature in numeric_feature if feature not in discrete_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_features.head()\n",
    "continuous_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete features (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the discrete feature and apply aggregation function on the target\n",
    "# In this case the function is the median, but can be also average, min, max, etc...\n",
    "\n",
    "for feature in discrete_features:\n",
    "    data = df_train.copy()\n",
    "    data.groupby(feature)[y_name].median().plot.bar()\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(y_name)\n",
    "    plt.title(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete features (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
